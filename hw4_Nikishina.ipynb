{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "from string import punctuation\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "%matplotlib inline\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import adagram\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAIN_DATA = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(PATH_TO_TRAIN_DATA, file) for file in os.listdir(PATH_TO_TRAIN_DATA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.concat([pd.read_json(file, lines=True, encoding='UTF-8') for file in files], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['norm'] = train_set['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['tok'] = train_set['content'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphraser/paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD, NMF, Word2Vec, Fastext, Adagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=3, max_df=0.5)#, max_features=1000)\n",
    "common = cv.fit_transform(train_set['norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.5) #, max_features=1000)\n",
    "common_tfidf = tfidf.fit_transform(train_set['norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, id2vec):\n",
    "    similar = [id2word[i] for i in cosine_distances(id2vec[word2id[word]].reshape(1, -1), id2vec).argsort()[0][:10]]\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i:w for i,w in enumerate(cv.get_feature_names())}\n",
    "word2id = {w:i for i,w in id2word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=200, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(200)\n",
    "svd.fit(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2vec_svd = svd.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['авария',\n",
       " 'дтп',\n",
       " 'кювета',\n",
       " 'салагай',\n",
       " 'кювет',\n",
       " 'бьянки',\n",
       " 'водитель',\n",
       " 'автомобиль',\n",
       " 'аварийность',\n",
       " 'неаварийный']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('авария', id2vec_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=200, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf = NMF(200)\n",
    "nmf.fit(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2vec_nmf = nmf.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['авария',\n",
       " 'разлагаться',\n",
       " 'метан',\n",
       " 'маг',\n",
       " 'безмолвие',\n",
       " 'каменистый',\n",
       " 'arco',\n",
       " 'скуп',\n",
       " 'поворот',\n",
       " 'микронный']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('авария', id2vec_nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec([text.split() for text in train_set['norm']], size=300, window=5, min_count=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('неполадка', 0.6342647075653076),\n",
       " ('гололёд', 0.6298475861549377),\n",
       " ('бьянки', 0.624162495136261),\n",
       " ('неисправность', 0.6012779474258423),\n",
       " ('поломка', 0.5839831829071045),\n",
       " ('возгорание', 0.5814708471298218),\n",
       " ('врезаться', 0.5809741020202637),\n",
       " ('дтп', 0.5802427530288696),\n",
       " ('фукусима-1', 0.5732443332672119),\n",
       " ('жюля', 0.562548041343689)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar('авария')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text = gensim.models.FastText([text.split() for text in train_set['tok']], size=100, min_n=3, max_n=6, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('аварии', 0.7561020851135254),\n",
       " ('аварий', 0.7436084151268005),\n",
       " ('бавария', 0.7136021852493286),\n",
       " ('посадка', 0.6971679329872131),\n",
       " ('мрия', 0.6920362710952759),\n",
       " ('эпилепсия', 0.6874281167984009),\n",
       " ('аварийность', 0.6747534275054932),\n",
       " ('аварию', 0.6707984805107117),\n",
       " ('осадка', 0.6693997383117676),\n",
       " ('подземка', 0.6646732091903687)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text.most_similar('авария')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_data_list = train_set[\"norm\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paraphraser/corpus_ada.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in common_data_list:\n",
    "        f.write(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!adagram-train paraphraser\\corpus_ada.txt paraphraser\\out.pkl --dim 100 --window 5 --epochs 5 --workers 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При первой же попытке это счастье зависло и погубило мне все тут. Больше я в ipython notebook так не развлекаюсь, потому запустила обучение из командной строки.\n",
    "А еще я хотела запустить на 5 эпохах, но мне выдали ошибку -- запускать больше чем на 1 эпохе -- это еще TODO :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = adagram.VectorModel.load(\"paraphraser/out2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.3705956947904682),\n",
       " (1, 0.31139444234191604),\n",
       " (2, 0.2665665698114904),\n",
       " (3, 0.051378498126314134)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.word_sense_probs('полиция')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('сведение', 1, 0.5179818),\n",
       " ('доставать', 0, 0.47486418),\n",
       " ('молодой', 1, 0.46848193),\n",
       " ('корреспондент', 1, 0.44809744),\n",
       " ('сокрытие', 0, 0.43769744),\n",
       " ('вокзал', 0, 0.43360248),\n",
       " ('человек', 4, 0.43116698),\n",
       " ('происшествие', 0, 0.41979373),\n",
       " ('рассчитать', 0, 0.4103935),\n",
       " ('бессмысленно', 0, 0.3845089)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.sense_neighbors('полиция', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('противодействовать', 0, 0.49866498),\n",
       " ('слабо', 0, 0.4482285),\n",
       " ('отток', 0, 0.43033817),\n",
       " ('алма-ата', 0, 0.42459294),\n",
       " ('обнаружить', 0, 0.37844193),\n",
       " ('фактор', 1, 0.37360293),\n",
       " ('молл', 0, 0.3576057),\n",
       " ('finabudapest2017', 0, 0.35505432),\n",
       " ('прошлое', 0, 0.34196287),\n",
       " ('усугубить', 0, 0.3387936)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.sense_neighbors('полиция', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Давайте проверим наши модельки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim):\n",
    "    text = text.split()\n",
    "    \n",
    "    # чтобы не доставать одно слово несколько раз\n",
    "    # сделаем счетчик, а потом векторы домножим на частоту\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            vectors[i] = v*(words[word]/total) # просто умножаем вектор на частоту\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adagram_embedding(text, model, window, dim):\n",
    "    text = text.split()\n",
    "    \n",
    "    \n",
    "    word2context = []\n",
    "    for i in range(len(text)-1):\n",
    "        left = max(0, i-window)\n",
    "        word = text[i]\n",
    "        left_context = text[left:i]\n",
    "        right_context = text[i+1:i+window]\n",
    "        context = left_context + right_context\n",
    "        word2context.append((word, context))\n",
    "    \n",
    "    \n",
    "    \n",
    "    vectors = np.zeros((len(word2context), dim))\n",
    "    \n",
    "    for i,word in enumerate(word2context):\n",
    "        word, context = word\n",
    "        try:\n",
    "            sense = model.disambiguate(word, context).argmax()\n",
    "            v = model.sense_vector(word, sense)\n",
    "            vectors[i] = v # просто умножаем вектор на частоту\n",
    "        \n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfolds(X_text, y, n_splits=5):\n",
    "    kf = KFold(n_splits)\n",
    "    f1_scores = []\n",
    "    for train_index, test_index in kf.split(X_text):\n",
    "        X_train, X_test = X_text[train_index], X_text[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        clf = svm.LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
    "                            intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "                            penalty='l2', random_state=None, tol=0.0001, verbose=0)\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "        f1_scores.append(f1_score(y_test, preds, average='micro'))\n",
    "    print(np.mean(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_random_state(X_text, y, random_state=1):\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(X_text, y, random_state=1)\n",
    "    clf = svm.LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
    "                        intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "                        penalty='l2', random_state=None, tol=0.0001, verbose=0)\n",
    "    clf.fit(train_X, train_y)\n",
    "    preds = clf.predict(valid_X)\n",
    "    print(classification_report(valid_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distances(X_1, X_2):\n",
    "    distances = []\n",
    "    for i in range(data.shape[0]):\n",
    "        distance = 1 - cosine_distances([X_1[i]], [X_2[i]])[0]\n",
    "        distances.append(distance)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svd_text_1 = svd.transform(cv.transform(data['text_1_norm']))\n",
    "X_svd_text_2 = svd.transform(cv.transform(data['text_2_norm']))\n",
    "\n",
    "X_svd_text = np.concatenate([X_svd_text_1, X_svd_text_2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_distance = generate_distances(X_svd_text_1, X_svd_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7227"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svd_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nmf_text_1 = nmf.transform(cv.transform(data['text_1_norm']))\n",
    "X_nmf_text_2 = nmf.transform(cv.transform(data['text_2_norm']))\n",
    "\n",
    "X_nmf_text = np.concatenate([X_nmf_text_1, X_nmf_text_2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_distance = generate_distances(X_nmf_text_1, X_nmf_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7227"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmf_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "dim = 300\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_distance = generate_distances(X_text_1_w2v, X_text_2_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7227"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "dim = 300\n",
    "data['text_1_notnorm'] = data['text_1'].apply(tokenize)\n",
    "data['text_2_notnorm'] = data['text_2'].apply(tokenize)\n",
    "\n",
    "X_text_1_ft = np.zeros((len(data['text_1_notnorm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_notnorm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_notnorm'].values):\n",
    "    X_text_1_ft[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_notnorm'].values):\n",
    "    X_text_2_ft[i] = get_embedding(text, fast_text, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fasttext_distance = generate_distances(X_text_1_ft, X_text_2_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7227"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fasttext_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\adagram\\model.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  z = np.log(z)\n"
     ]
    }
   ],
   "source": [
    "dim = 100\n",
    "X_text_ada_1 = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_ada_2 = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_ada_1[i] = get_adagram_embedding(text, vm, 5, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_ada_2[i] = get_adagram_embedding(text, vm, 5, dim)\n",
    "\n",
    "X_ada_text = np.concatenate([X_text_ada_1, X_text_ada_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_distance = generate_distances(X_text_ada_1, X_text_ada_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7227"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ada_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_distances = np.concatenate([svd_distance, nmf_distance, w2v_distance, fasttext_distance, ada_distance], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49039110661563273\n"
     ]
    }
   ],
   "source": [
    "run_kfolds(X_distances, y, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.52      0.83      0.64       629\n",
      "          0       0.50      0.28      0.35       737\n",
      "          1       0.49      0.43      0.46       441\n",
      "\n",
      "avg / total       0.50      0.51      0.48      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_on_random_state(X_distances, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Давайте что-нибудь улучшим"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С фантазией у меня туговато, поэтому давайте попробуем 2 вещи:\n",
    "* попробуем добавить чатси речи к леммам для нашего w2v\n",
    "* попробуем использовать дополнительно w2v, обученный на новостных статьях из RusVectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_pos(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form+\"_\"+str(word.tag.POS) for word in words if word]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['pos'] = train_set['content'].apply(define_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_pos'] = data['text_1'].apply(define_pos)\n",
    "data['text_2_pos'] = data['text_2'].apply(define_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_pos = gensim.models.Word2Vec([text.split() for text in train_set['pos']], size=300, window=5, min_count=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('гололёд_NOUN', 0.6457265019416809),\n",
       " ('бьянки_NOUN', 0.6411608457565308),\n",
       " ('неполадка_NOUN', 0.6307287216186523),\n",
       " ('дтп_NOUN', 0.6277387142181396),\n",
       " ('неисправность_NOUN', 0.6102908849716187),\n",
       " ('врезаться_VERB', 0.5891246795654297),\n",
       " ('жюля_NOUN', 0.5794494152069092),\n",
       " ('обледенение_NOUN', 0.5678970813751221),\n",
       " ('караченцов_NOUN', 0.5654643774032593),\n",
       " ('пит-лейн_NOUN', 0.5645472407341003)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pos.most_similar('авария_NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "dim = 300\n",
    "X_text_1_w2v_pos = np.zeros((len(data['text_1_pos']), dim))\n",
    "X_text_2_w2v_pos = np.zeros((len(data['text_2_pos']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_pos'].values):\n",
    "    X_text_1_w2v_pos[i] = get_embedding(text, w2v_pos, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_pos'].values):\n",
    "    X_text_2_w2v_pos[i] = get_embedding(text, w2v_pos, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_pos_distance = generate_distances(X_text_1_w2v_pos, X_text_2_w2v_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец-то мы добрались до самого интересного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_distances = np.concatenate([svd_distance, nmf_distance, w2v_distance, fasttext_distance, w2v_pos_distance, ada_distance], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5045099098477795\n"
     ]
    }
   ],
   "source": [
    "run_kfolds(X_distances, y, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.52      0.82      0.63       629\n",
      "          0       0.47      0.49      0.48       737\n",
      "          1       0.42      0.03      0.06       441\n",
      "\n",
      "avg / total       0.47      0.49      0.43      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_on_random_state(X_distances, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что будет, если использовать только один w2v_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_distances = np.concatenate([svd_distance, nmf_distance, fasttext_distance, w2v_pos_distance, ada_distance], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5035271625230361\n"
     ]
    }
   ],
   "source": [
    "run_kfolds(X_distances, y, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.52      0.83      0.64       629\n",
      "          0       0.50      0.23      0.32       737\n",
      "          1       0.46      0.48      0.47       441\n",
      "\n",
      "avg / total       0.50      0.50      0.47      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_on_random_state(X_distances, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мне кажется, это очень даже неплохо =/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec news_upos_cbow_600_2_2018 from RusVectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = 'news_upos_cbow_600_2_2018.vec.gz'\n",
    "big_model = gensim.models.KeyedVectors.load_word2vec_format(modelfile, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('дтп_NOUN', 0.7848729491233826),\n",
       " ('автоавария_NOUN', 0.7186106443405151),\n",
       " ('автокатастрофа_NOUN', 0.6146150827407837),\n",
       " ('чп_NOUN', 0.608892560005188),\n",
       " ('происшествие_NOUN', 0.6018447875976562),\n",
       " ('инцидент_NOUN', 0.5716761946678162),\n",
       " ('автопроисшествие_NOUN', 0.5431926250457764),\n",
       " ('авиакатастрофа_NOUN', 0.5024130344390869),\n",
       " ('трагедия_NOUN', 0.5010612607002258),\n",
       " ('авиапроисшествие_NOUN', 0.4828478693962097)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_model.most_similar('авария_NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ветхий_ADJ', 0.5710058212280273),\n",
       " ('нештатный_ADJ', 0.4783197343349457),\n",
       " ('аварийна_NOUN', 0.4630982577800751),\n",
       " ('аварийний_ADJ', 0.442495733499527),\n",
       " ('аварийщик_NOUN', 0.4103148877620697),\n",
       " ('ветхоаварийный_ADJ', 0.4031134247779846),\n",
       " ('аварийно-ремонтный_ADJ', 0.38955214619636536),\n",
       " ('ветхость_NOUN', 0.3825417160987854),\n",
       " ('экстренный_ADJ', 0.3801092803478241),\n",
       " ('негодный_ADJ', 0.37957727909088135)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_model.most_similar('аварийный_ADJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как теги pymorphy и Universal Tags не совпадают (а это влияет на качество, я проверила :( ), то давайте исправим теги =/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_tags(text):\n",
    "    return text.replace('ADJF', 'ADJ').replace('ADJS', \"ADJ\").replace('NPRO', 'PROPN').replace('NUMR', 'NUM').replace('ADVB', 'ADV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_pos_norm'] = data['text_1_pos'].apply(correct_tags)\n",
    "data['text_2_pos_norm'] = data['text_2_pos'].apply(correct_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 600\n",
    "X_text_1_w2v_big = np.zeros((len(data['text_1_pos_norm']), dim))\n",
    "X_text_2_w2v_big = np.zeros((len(data['text_2_pos_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_pos_norm'].values):\n",
    "    X_text_1_w2v_big[i] = get_embedding(text, big_model, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_pos_norm'].values):\n",
    "    X_text_2_w2v_big[i] = get_embedding(text, big_model, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_big_distance = generate_distances(X_text_1_w2v_big, X_text_2_w2v_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, улучшится ли качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_distances = np.concatenate([svd_distance, nmf_distance, w2v_distance, fasttext_distance, w2v_big_distance, ada_distance], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51626533028356\n"
     ]
    }
   ],
   "source": [
    "run_kfolds(X_distances, y, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.52      0.83      0.64       629\n",
      "          0       0.47      0.38      0.42       737\n",
      "          1       0.49      0.23      0.31       441\n",
      "\n",
      "avg / total       0.49      0.50      0.47      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_on_random_state(X_distances, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А что будет, если использовать только RusVectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_distances = np.concatenate([svd_distance, nmf_distance, fasttext_distance, w2v_big_distance, ada_distance], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4848515151166845\n"
     ]
    }
   ],
   "source": [
    "run_kfolds(X_distances, y, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.51      0.82      0.63       629\n",
      "          0       0.48      0.31      0.37       737\n",
      "          1       0.52      0.38      0.44       441\n",
      "\n",
      "avg / total       0.50      0.50      0.48      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_on_random_state(X_distances, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или вообще только его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_distances = np.concatenate([w2v_big_distance], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5025664455964106\n"
     ]
    }
   ],
   "source": [
    "run_kfolds(X_distances, y, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.51      0.83      0.63       629\n",
      "          0       0.47      0.03      0.06       737\n",
      "          1       0.42      0.70      0.53       441\n",
      "\n",
      "avg / total       0.47      0.47      0.37      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_on_random_state(X_distances, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: не могу сказать, что мне очень нравится результат, но, в любом случае, эмбеддинги -- это любовь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
